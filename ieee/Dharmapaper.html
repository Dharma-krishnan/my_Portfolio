<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=fpjTOVmNbO4Lz34iLyptLUXza5VhXqVC6o75Eld_V98);.lst-kix_list_4-1>li{counter-increment:lst-ctn-kix_list_4-1}.lst-kix_list_2-1>li{counter-increment:lst-ctn-kix_list_2-1}ol.lst-kix_list_3-1.start{counter-reset:lst-ctn-kix_list_3-1 0}ol.lst-kix_6pg1c0ycu0qm-3.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-3 0}ol.lst-kix_list_2-3.start{counter-reset:lst-ctn-kix_list_2-3 0}.lst-kix_c3khh8kwucpb-7>li:before{content:"\0025cb   "}.lst-kix_c3khh8kwucpb-8>li:before{content:"\0025a0   "}ol.lst-kix_list_1-5.start{counter-reset:lst-ctn-kix_list_1-5 0}.lst-kix_list_2-3>li{counter-increment:lst-ctn-kix_list_2-3}.lst-kix_list_4-3>li{counter-increment:lst-ctn-kix_list_4-3}ol.lst-kix_list_4-5.start{counter-reset:lst-ctn-kix_list_4-5 0}.lst-kix_6pg1c0ycu0qm-0>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-0,decimal) ". "}.lst-kix_list_1-2>li{counter-increment:lst-ctn-kix_list_1-2}ol.lst-kix_list_3-7.start{counter-reset:lst-ctn-kix_list_3-7 0}.lst-kix_6pg1c0ycu0qm-1>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-1,lower-latin) ". "}.lst-kix_6pg1c0ycu0qm-4>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-4,lower-latin) ". "}.lst-kix_6pg1c0ycu0qm-2>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-2,lower-roman) ". "}.lst-kix_list_3-2>li{counter-increment:lst-ctn-kix_list_3-2}.lst-kix_6pg1c0ycu0qm-3>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-3,decimal) ". "}ol.lst-kix_6pg1c0ycu0qm-1{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-2{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-3{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-4{list-style-type:none}.lst-kix_list_1-4>li{counter-increment:lst-ctn-kix_list_1-4}ol.lst-kix_6pg1c0ycu0qm-0{list-style-type:none}ol.lst-kix_list_1-6.start{counter-reset:lst-ctn-kix_list_1-6 0}ol.lst-kix_6pg1c0ycu0qm-5{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-6{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-7{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-8{list-style-type:none}ol.lst-kix_list_1-0.start{counter-reset:lst-ctn-kix_list_1-0 0}.lst-kix_list_3-0>li{counter-increment:lst-ctn-kix_list_3-0}ol.lst-kix_list_4-0.start{counter-reset:lst-ctn-kix_list_4-0 0}.lst-kix_list_3-6>li{counter-increment:lst-ctn-kix_list_3-6}ol.lst-kix_6pg1c0ycu0qm-4.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-4 0}.lst-kix_list_2-5>li{counter-increment:lst-ctn-kix_list_2-5}.lst-kix_list_2-8>li{counter-increment:lst-ctn-kix_list_2-8}ol.lst-kix_list_3-2.start{counter-reset:lst-ctn-kix_list_3-2 0}.lst-kix_6pg1c0ycu0qm-1>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-1}ol.lst-kix_list_2-4.start{counter-reset:lst-ctn-kix_list_2-4 0}ol.lst-kix_list_1-3{list-style-type:none}ol.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_2-7>li:before{content:"" counter(lst-ctn-kix_list_2-7,lower-latin) ". "}.lst-kix_list_2-7>li{counter-increment:lst-ctn-kix_list_2-7}ol.lst-kix_list_1-5{list-style-type:none}ol.lst-kix_list_1-6{list-style-type:none}ol.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_2-5>li:before{content:"" counter(lst-ctn-kix_list_2-5,lower-roman) ". "}ol.lst-kix_list_1-1{list-style-type:none}ol.lst-kix_list_1-2{list-style-type:none}ol.lst-kix_list_4-6.start{counter-reset:lst-ctn-kix_list_4-6 0}ol.lst-kix_list_3-0.start{counter-reset:lst-ctn-kix_list_3-0 0}ol.lst-kix_list_4-3.start{counter-reset:lst-ctn-kix_list_4-3 0}ol.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_4-7>li{counter-increment:lst-ctn-kix_list_4-7}ol.lst-kix_list_1-8{list-style-type:none}ol.lst-kix_list_2-5.start{counter-reset:lst-ctn-kix_list_2-5 0}.lst-kix_list_2-6>li{counter-increment:lst-ctn-kix_list_2-6}.lst-kix_list_4-1>li:before{content:" "}.lst-kix_list_4-3>li:before{content:" "}.lst-kix_list_4-5>li:before{content:" "}.lst-kix_list_1-8>li{counter-increment:lst-ctn-kix_list_1-8}ol.lst-kix_list_1-4.start{counter-reset:lst-ctn-kix_list_1-4 0}.lst-kix_list_3-5>li{counter-increment:lst-ctn-kix_list_3-5}ol.lst-kix_list_1-1.start{counter-reset:lst-ctn-kix_list_1-1 0}.lst-kix_list_3-4>li{counter-increment:lst-ctn-kix_list_3-4}ol.lst-kix_list_4-4.start{counter-reset:lst-ctn-kix_list_4-4 0}.lst-kix_6pg1c0ycu0qm-5>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-5,lower-roman) ". "}.lst-kix_6pg1c0ycu0qm-3>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-3}.lst-kix_6pg1c0ycu0qm-7>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-7,lower-latin) ". "}.lst-kix_6pg1c0ycu0qm-8>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-8}.lst-kix_6pg1c0ycu0qm-2>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-2}ol.lst-kix_list_1-3.start{counter-reset:lst-ctn-kix_list_1-3 0}ol.lst-kix_list_2-8.start{counter-reset:lst-ctn-kix_list_2-8 0}ol.lst-kix_list_1-2.start{counter-reset:lst-ctn-kix_list_1-2 0}ol.lst-kix_6pg1c0ycu0qm-8.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-8 0}.lst-kix_list_1-1>li:before{content:"" counter(lst-ctn-kix_list_1-1,lower-latin) ". "}.lst-kix_list_1-3>li:before{content:"" counter(lst-ctn-kix_list_1-3,decimal) ". "}.lst-kix_list_4-8>li{counter-increment:lst-ctn-kix_list_4-8}.lst-kix_list_1-7>li:before{content:"" counter(lst-ctn-kix_list_1-7,lower-latin) ". "}ol.lst-kix_list_2-7.start{counter-reset:lst-ctn-kix_list_2-7 0}.lst-kix_c3khh8kwucpb-5>li:before{content:"\0025a0   "}.lst-kix_list_1-3>li{counter-increment:lst-ctn-kix_list_1-3}.lst-kix_list_1-5>li:before{content:"" counter(lst-ctn-kix_list_1-5,lower-roman) ". "}ol.lst-kix_6pg1c0ycu0qm-7.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-7 0}.lst-kix_c3khh8kwucpb-3>li:before{content:"\0025cf   "}.lst-kix_list_2-1>li:before{content:"" counter(lst-ctn-kix_list_2-1,lower-latin) ". "}.lst-kix_list_2-3>li:before{content:"" counter(lst-ctn-kix_list_2-3,decimal) ". "}.lst-kix_c3khh8kwucpb-1>li:before{content:"\0025cf   "}.lst-kix_list_4-2>li{counter-increment:lst-ctn-kix_list_4-2}ol.lst-kix_list_3-1{list-style-type:none}ol.lst-kix_list_3-2{list-style-type:none}.lst-kix_list_3-1>li{counter-increment:lst-ctn-kix_list_3-1}ol.lst-kix_list_3-3{list-style-type:none}ol.lst-kix_list_3-4.start{counter-reset:lst-ctn-kix_list_3-4 0}ol.lst-kix_list_3-4{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-6.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-6 0}ol.lst-kix_list_3-0{list-style-type:none}.lst-kix_list_1-1>li{counter-increment:lst-ctn-kix_list_1-1}.lst-kix_6pg1c0ycu0qm-0>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-0}ol.lst-kix_list_2-6.start{counter-reset:lst-ctn-kix_list_2-6 0}.lst-kix_list_3-0>li:before{content:"" counter(lst-ctn-kix_list_3-0,upper-roman) ". "}.lst-kix_list_3-1>li:before{content:"" counter(lst-ctn-kix_list_3-1,lower-latin) ". "}.lst-kix_list_3-2>li:before{content:"" counter(lst-ctn-kix_list_3-2,lower-roman) ". "}ol.lst-kix_list_1-8.start{counter-reset:lst-ctn-kix_list_1-8 0}.lst-kix_list_4-0>li{counter-increment:lst-ctn-kix_list_4-0}.lst-kix_list_3-5>li:before{content:"" counter(lst-ctn-kix_list_3-5,lower-roman) ". "}.lst-kix_list_3-4>li:before{content:"" counter(lst-ctn-kix_list_3-4,lower-latin) ". "}.lst-kix_list_3-3>li:before{content:"" counter(lst-ctn-kix_list_3-3,decimal) ". "}ol.lst-kix_list_3-5{list-style-type:none}ol.lst-kix_list_3-6{list-style-type:none}ol.lst-kix_list_3-7{list-style-type:none}ol.lst-kix_list_3-8{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-0.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-0 0}.lst-kix_list_3-8>li:before{content:"" counter(lst-ctn-kix_list_3-8,lower-roman) ". "}.lst-kix_list_2-0>li{counter-increment:lst-ctn-kix_list_2-0}.lst-kix_list_3-6>li:before{content:"" counter(lst-ctn-kix_list_3-6,decimal) ". "}.lst-kix_list_3-7>li:before{content:"" counter(lst-ctn-kix_list_3-7,lower-latin) ". "}ol.lst-kix_list_4-2.start{counter-reset:lst-ctn-kix_list_4-2 0}ol.lst-kix_list_2-2{list-style-type:none}ol.lst-kix_list_2-3{list-style-type:none}ol.lst-kix_list_2-4{list-style-type:none}ol.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_4-4>li{counter-increment:lst-ctn-kix_list_4-4}ol.lst-kix_list_2-0{list-style-type:none}ol.lst-kix_list_2-1{list-style-type:none}.lst-kix_list_4-8>li:before{content:" "}.lst-kix_list_4-7>li:before{content:" "}ol.lst-kix_list_4-1.start{counter-reset:lst-ctn-kix_list_4-1 0}ol.lst-kix_list_4-8.start{counter-reset:lst-ctn-kix_list_4-8 0}ol.lst-kix_6pg1c0ycu0qm-5.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-5 0}ol.lst-kix_list_3-3.start{counter-reset:lst-ctn-kix_list_3-3 0}ol.lst-kix_list_2-6{list-style-type:none}ol.lst-kix_list_2-7{list-style-type:none}ol.lst-kix_list_2-8{list-style-type:none}.lst-kix_list_3-3>li{counter-increment:lst-ctn-kix_list_3-3}.lst-kix_6pg1c0ycu0qm-7>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-7}.lst-kix_6pg1c0ycu0qm-4>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-4}.lst-kix_list_2-2>li{counter-increment:lst-ctn-kix_list_2-2}ol.lst-kix_list_4-7.start{counter-reset:lst-ctn-kix_list_4-7 0}.lst-kix_list_2-6>li:before{content:"" counter(lst-ctn-kix_list_2-6,decimal) ". "}.lst-kix_list_3-7>li{counter-increment:lst-ctn-kix_list_3-7}.lst-kix_list_2-4>li:before{content:"" counter(lst-ctn-kix_list_2-4,lower-latin) ". "}.lst-kix_list_2-8>li:before{content:"" counter(lst-ctn-kix_list_2-8,lower-roman) ". "}.lst-kix_6pg1c0ycu0qm-6>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-6}.lst-kix_6pg1c0ycu0qm-5>li{counter-increment:lst-ctn-kix_6pg1c0ycu0qm-5}.lst-kix_list_1-7>li{counter-increment:lst-ctn-kix_list_1-7}ol.lst-kix_list_3-8.start{counter-reset:lst-ctn-kix_list_3-8 0}ul.lst-kix_c3khh8kwucpb-0{list-style-type:none}ul.lst-kix_c3khh8kwucpb-1{list-style-type:none}ul.lst-kix_c3khh8kwucpb-2{list-style-type:none}ul.lst-kix_c3khh8kwucpb-3{list-style-type:none}ul.lst-kix_c3khh8kwucpb-4{list-style-type:none}ul.lst-kix_c3khh8kwucpb-5{list-style-type:none}ul.lst-kix_c3khh8kwucpb-6{list-style-type:none}ul.lst-kix_c3khh8kwucpb-7{list-style-type:none}.lst-kix_list_4-0>li:before{content:" "}.lst-kix_list_3-8>li{counter-increment:lst-ctn-kix_list_3-8}ol.lst-kix_6pg1c0ycu0qm-2.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-2 0}.lst-kix_list_4-6>li{counter-increment:lst-ctn-kix_list_4-6}ol.lst-kix_list_1-7.start{counter-reset:lst-ctn-kix_list_1-7 0}.lst-kix_list_4-4>li:before{content:" "}ol.lst-kix_list_2-2.start{counter-reset:lst-ctn-kix_list_2-2 0}.lst-kix_list_1-5>li{counter-increment:lst-ctn-kix_list_1-5}.lst-kix_list_4-2>li:before{content:" "}.lst-kix_list_4-6>li:before{content:" "}ul.lst-kix_c3khh8kwucpb-8{list-style-type:none}ol.lst-kix_list_4-0{list-style-type:none}.lst-kix_6pg1c0ycu0qm-8>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-8,lower-roman) ". "}ol.lst-kix_list_4-1{list-style-type:none}ol.lst-kix_list_4-2{list-style-type:none}ol.lst-kix_list_4-3{list-style-type:none}ol.lst-kix_6pg1c0ycu0qm-1.start{counter-reset:lst-ctn-kix_6pg1c0ycu0qm-1 0}.lst-kix_list_2-4>li{counter-increment:lst-ctn-kix_list_2-4}ol.lst-kix_list_3-6.start{counter-reset:lst-ctn-kix_list_3-6 0}.lst-kix_6pg1c0ycu0qm-6>li:before{content:"" counter(lst-ctn-kix_6pg1c0ycu0qm-6,decimal) ". "}ol.lst-kix_list_4-8{list-style-type:none}.lst-kix_list_1-0>li:before{content:"" counter(lst-ctn-kix_list_1-0,decimal) ". "}ol.lst-kix_list_4-4{list-style-type:none}ol.lst-kix_list_4-5{list-style-type:none}.lst-kix_list_1-2>li:before{content:"" counter(lst-ctn-kix_list_1-2,lower-roman) ". "}ol.lst-kix_list_2-0.start{counter-reset:lst-ctn-kix_list_2-0 0}ol.lst-kix_list_4-6{list-style-type:none}ol.lst-kix_list_4-7{list-style-type:none}.lst-kix_list_1-4>li:before{content:"" counter(lst-ctn-kix_list_1-4,lower-latin) ". "}ol.lst-kix_list_3-5.start{counter-reset:lst-ctn-kix_list_3-5 0}.lst-kix_list_1-0>li{counter-increment:lst-ctn-kix_list_1-0}.lst-kix_list_1-6>li{counter-increment:lst-ctn-kix_list_1-6}.lst-kix_c3khh8kwucpb-6>li:before{content:"\0025cf   "}.lst-kix_list_1-6>li:before{content:"" counter(lst-ctn-kix_list_1-6,decimal) ". "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_c3khh8kwucpb-2>li:before{content:"\0025a0   "}.lst-kix_c3khh8kwucpb-4>li:before{content:"\0025cb   "}.lst-kix_list_2-0>li:before{content:"" counter(lst-ctn-kix_list_2-0,decimal) ". "}ol.lst-kix_list_2-1.start{counter-reset:lst-ctn-kix_list_2-1 0}.lst-kix_list_4-5>li{counter-increment:lst-ctn-kix_list_4-5}.lst-kix_list_1-8>li:before{content:"" counter(lst-ctn-kix_list_1-8,lower-roman) ". "}.lst-kix_list_2-2>li:before{content:"" counter(lst-ctn-kix_list_2-2,lower-roman) ". "}.lst-kix_c3khh8kwucpb-0>li:before{content:"  "}ol{margin:0;padding:0}table td,table th{padding:0}.c22{margin-left:36pt;padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left;height:11pt}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Times New Roman";font-style:italic}.c36{margin-left:36pt;padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c29{margin-left:36pt;padding-top:1pt;padding-bottom:1pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c6{padding-top:0pt;padding-bottom:10pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left;height:11pt}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:center}.c20{padding-top:0pt;padding-bottom:10pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c38{padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:justify}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c19{padding-top:12pt;padding-bottom:8pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c1{padding-top:0pt;padding-bottom:10pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:justify}.c32{padding-top:12pt;padding-bottom:8pt;line-height:1.5;orphans:2;widows:2;text-align:justify}.c13{padding-top:0pt;padding-bottom:10pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:center}.c7{background-color:#ffffff;font-size:12pt;font-family:"Times New Roman";color:#222222;font-weight:400}.c0{font-size:10pt;font-family:"Times New Roman";font-style:italic;font-weight:400}.c17{font-size:9pt;font-family:"Times New Roman";font-style:italic;font-weight:400}.c2{font-size:9pt;font-family:"Times New Roman";font-style:italic;font-weight:700}.c39{font-weight:700;font-size:21pt;font-family:"Times New Roman"}.c37{font-weight:400;font-size:11pt;font-family:"Calibri"}.c34{font-family:"Times New Roman";color:#1155cc;font-weight:400}.c28{font-size:8pt;font-family:"Times New Roman";font-weight:700}.c15{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c27{font-weight:700;font-size:14pt;font-family:"Times New Roman"}.c9{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c43{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c16{font-size:12pt;font-family:"Arial";font-weight:400}.c21{vertical-align:super;font-family:"Times New Roman";font-weight:700}.c42{font-weight:400;font-size:16pt;font-family:"Times New Roman"}.c14{font-size:13pt;font-family:"Times New Roman";font-weight:700}.c11{text-decoration:none;vertical-align:baseline;font-style:normal}.c44{font-weight:400;font-size:14pt;font-family:"Times New Roman"}.c31{background-color:#ffffff;max-width:552.1pt;padding:21.6pt 21.6pt 21.6pt 21.6pt}.c41{font-size:18pt;font-family:"Times New Roman";font-weight:700}.c25{color:inherit;text-decoration:inherit}.c35{padding:0;margin:0}.c26{text-decoration:none;vertical-align:baseline}.c33{text-decoration:none;font-style:normal}.c10{margin-left:36pt;padding-left:0pt}.c40{margin-left:36pt}.c12{color:#000000}.c18{font-style:italic}.c45{font-size:16pt}.c23{height:11pt}.c24{font-size:13pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Calibri";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.1500000000000001;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Calibri"}p{margin:0;color:#000000;font-size:11pt;font-family:"Calibri"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Calibri";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Calibri";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Calibri";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.body {margin: 0;padding: 0;display: flex;justify-content: center;align-items: center;min-height: 100vh;background-color: #f0f0f0;font-family: Arial, sans-serif;}</style></head><body class="c31 content doc-content"><div><p class="c6"><span class="c37 c11 c12"></span></p></div><p class="c20"><span class="c41">Integrated Facial Attribute Analysis: A Comprehensive Approach for Face Mask Detection, Age Prediction and Gender Classification</span></p><p class="c29"><span class="c14">R. DHARMA </span><span class="c14">KRISHNAN</span><span class="c21 c24">1</span><span class="c21 c24">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c14">Dr.ILAM CHEZHIAN .J</span><span class="c21 c24">&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c14">SARALA DEVI.V</span><span class="c21 c24">&nbsp;3</span></p><p class="c36"><span class="c0">MCA Student</span><span class="c18 c21">1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0">Assistant Professor </span><span class="c21 c18">2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0">Assistant Professor </span><span class="c21 c18">3</span></p><p class="c36"><span class="c34 c18 c43"><a class="c25" href="mailto:dharmab2001@gmail.com">dharmab2001@gmail.com</a></span><span class="c15 c18">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c34 c18"><a class="c25" href="mailto:Ilamchezhian.mca@drmgrdu.ac.in">Ilamchezhian.mca@drmgrdu.ac.in</a></span><span class="c15 c18">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c18 c34"><a class="c25" href="mailto:saraladevi.mca@drmgrdu.ac.in">saraladevi.mca@drmgrdu.ac.in</a></span></p><p class="c38"><span class="c26 c12 c18 c44">Department of Computer Applications,<br>Dr. M.G.R Educational And Research Institute,Chennai, Tamil Nadu- 600095.</span></p><p class="c22"><span class="c3"></span></p><p class="c4"><span class="c27 c11 c12">ABSTRACT</span></p><p class="c4"><span class="c3">In today&#39;s world, keeping people healthy and safe requires smart technology. This research introduces a strong system that can detect if someone is wearing a face mask in real-time. It uses advanced computer vision and deep learning techniques. The main goal of this system is to quickly tell if someone in a live video is wearing a mask or not. The heart of the system is a deep learning model based on ResNet. It&#39;s been carefully trained to recognize facial features and accurately tell if someone is wearing a mask. This model works really well, reliably spotting people following mask rules and those who aren&#39;t. Additionally, the system can estimate the gender and age of people, which adds useful information. To make it easy for users to interact with the system, a Graphical User Interface (GUI) has been created using the Tkinter framework. This simple interface lets users easily use the system, seeing real-time results of mask detection.</span></p><p class="c4 c23"><span class="c3"></span></p><p class="c1"><span class="c9">Keywords</span><span class="c27">: </span><span class="c15">Face mask detection, Computer vision, Deep learning, &nbsp;Real-time classification, ResNet, Gender classification</span><span class="c16">.</span></p><p class="c30"><span class="c27">INTRODUCTION</span></p><p class="c4"><span class="c3">In today&#39;s world, security and safety are paramount concerns in various real-world scenarios, such as surveillance, law enforcement, and public safety. One critical aspect of ensuring security is the ability to monitor and enforce compliance with safety regulations, including the use of personal protective equipment (PPE) such as face masks. The development of automated systems for face mask detection has become increasingly important in these contexts as they offer practical solutions for enhancing security measures.</span></p><p class="c1"><span class="c15">This project focuses on the development of a real-time face mask detection system using ResNet-based deep learning for accurate mask classification. The system is designed for deployment in environments where ensuring compliance with safety regulations is critical, such as airports, hospitals, and public transportation. By automating the process of monitoring mask-wearing behavior, the system aims to improve security and safety standards in these settings. The system&#39;s user-friendly interface, developed using Tkinter, allows for easy interaction and integration with existing security systems. Additionally, the system provides gender and age estimation for detected faces, which can be valuable for demographic analysis and profiling. Overall, this project addresses the need for effective security solutions in various real-world scenarios, contributing to the enhancement of public safety and security standards.</span></p><p class="c8"><span class="c9 c11 c12">LITERATURE </span><span class="c9">SURVEY</span></p><p class="c4" id="h.1fob9te"><span class="c3">Arora et al. (2023) delve into computer vision, focusing on object detection, age estimation, and gender estimation using deep learning. Their study introduces a comprehensive framework utilizing advanced models like Mask R-CNN and the Deep Face library. By integrating these components, the research demonstrates precise object detection and nuanced age and gender estimations. The authors highlight their model&#39;s efficiency through meticulous information processing, resulting in high-quality output images. Rigorous testing validates the accurate identification of objects and precise estimation of age and gender, advancing computer vision and deep learning applications.</span></p><p class="c4 c23" id="h.8f3084slevq7"><span class="c3"></span></p><p class="c1" id="h.3xy65t3i8efk"><span class="c15">Krishnakumar et al. (2022) responds to the critical need for effective measures in combating COVID-19 and safeguarding public health. Their study focuses on developing precise techniques for detecting non-compliance with mask-wearing protocols in public areas. Leveraging MobileNet V2 as a foundation and utilizing transfer learning methods, the researchers enhance the model&#39;s accuracy in mask recognition. By integrating the Caffe Model with OpenCV&#39;s DNN module for face detection, their anticipated model exhibits exceptional performance suitable for real-time applications, particularly in live video surveillance systems for monitoring mask adherence.</span></p><p class="c1"><span class="c3">Banati et al. (2022) present a pioneering approach to integrating soft biometrics and deep learning techniques to recognize facial features in masked images. Their study addresses the pressing need for efficient identification methods amidst the COVID-19 pandemic. They introduce a novel system that leverages ocular and forehead regions to detect soft biometric attributes like eyeglasses, hair type, and mustache. Employing transfer learning techniques, they achieve impressive accuracy rates, even with facial masks present. Their comparisons between masked and unmasked face images demonstrate the effectiveness of the proposed system, particularly highlighting the enhanced accuracy of MobileNet. This research significantly advances facial recognition technologies, particularly in overcoming challenges posed by mask mandates during the pandemic.</span></p><p class="c1"><span class="c3">Wang, L., Zhang, Q., Chen, Y (2020) Focusing on privacy concerns related to facial recognition, this research investigates methods for enhancing privacy protection in facial recognition systems. The study explores approaches such as facial obfuscation and privacy-preserving feature extraction, providing insights into strategies for mitigating privacy risks in facial recognition technology.</span></p><p class="c1"><span class="c3">Patel, K., Lee, J., and Smith, A. (2019) provide insights into facial recognition systems for security applications. The paper discusses traditional methods alongside recent advancements in deep learning, examining techniques, challenges, and ethical considerations associated with facial recognition technology.</span></p><p class="c1"><span class="c3">Gupta, A., Singh, R., and Sharma, S. (2021) investigate real-time face mask detection using YOLOv3 and transfer learning. Their study aims to fine-tune pre-trained YOLOv3 models for efficient monitoring of mask compliance in various settings.</span></p><p class="c1"><span class="c3">Williams, E., Johnson, M., &amp; Brown, K. (2021) delve into the development of AI-based systems for face mask detection in public spaces. Their research explores the integration of machine learning algorithms with IoT devices to enhance real-time monitoring of mask compliance, aiming to contribute to public health efforts amidst the COVID-19 pandemic.</span></p><p class="c1"><span class="c3">Chen, H., Li, W., &amp; Liu, J. (2018) present a comprehensive review of deep learning techniques for facial recognition and attribute analysis. Their study evaluates the performance of various deep learning architectures and discusses challenges and future directions in the field of facial recognition and attribute prediction.</span></p><p class="c1"><span class="c3">Garcia, P., Martinez, L., &amp; Rodriguez, A. (2020) investigate the application of facial recognition technology in healthcare settings. Their research explores the potential of facial recognition systems for patient identification, access control, and personalized healthcare delivery, emphasizing the importance of accuracy, security, and privacy in healthcare-related applications of facial recognition.</span></p><p class="c1"><span class="c15">Jones, R., Smith, T., &amp; Patel, A. (2023) explore the use of machine learning algorithms for analyzing compliance with face mask mandates in public settings. Their study investigates the effectiveness of various deep learning models in accurately detecting individuals wearing masks and provides insights into the deployment of such systems for enhancing public safety measures.</span></p><p class="c8"><span class="c27">PROPOSED SYSTEM</span></p><p class="c4"><span class="c3">The proposed system aims to augment existing mask detection capabilities by integrating age and gender prediction functionalities, thus offering a comprehensive solution for real-time monitoring of mask compliance. Leveraging ResNet for precise mask and face detection, supplemented by pre-trained models for age and gender classification, the system is poised to significantly enhance the efficacy of mask detection processes. </span></p><p class="c1"><span class="c3">By incorporating ResNet for face and mask detection, the proposed system anticipates achieving superior accuracy and performance compared to existing approaches. The utilization of ResNet, a robust deep learning architecture, enables the system to effectively identify and classify facial features, thereby facilitating more precise mask detection.</span></p><p class="c1"><span class="c3">The primary objective of the proposed system is to provide a comprehensive solution for real-time face mask detection, age estimation, and gender prediction. In environments where mask-wearing is mandatory, such as healthcare facilities or public transportation hubs, the system can enforce compliance by accurately identifying individuals not wearing masks. This proactive approach contributes to broader efforts aimed at mitigating the spread of infectious diseases and ensuring public safety in restricted areas.</span></p><p class="c1"><span class="c15">Furthermore, in retail settings, the proposed system can offer valuable insights into the demographics of customers, including age and gender. This demographic information empowers businesses to tailor their products and marketing strategies effectively, thereby enhancing customer engagement and satisfaction.</span></p><p class="c13"><span class="c27">ARCHITECTURE DIAGRAM</span><span class="c3">&nbsp; </span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 356.70px; height: 174.54px;"><img alt="" src="images/image8.png" style="width: 356.70px; height: 174.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c15">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c13"><span class="c2">Figure 1:</span><span class="c17">&nbsp;Real-Time Face Mask, Age and Gender Prediction Architecture</span></p><p class="c8"><span class="c9">FACE DETECTION</span></p><p class="c4"><span class="c3">The Face Detection Module plays a crucial role in the system by identifying and localizing faces within a &nbsp;video frame. This module typically employs a pre-trained deep learning model, such as a ResNet, to detect facial features and determine the bounding boxes around detected faces. The deep learning model is trained on a dataset of labeled face images to learn the patterns and features that represent faces, enabling it to accurately detect faces in various orientations, lighting conditions, and environments. The output of this module is a set of bounding boxes that define the location of each detected face, which is then passed on to the subsequent modules for further processing.</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 266.00px; height: 148.50px;"><img alt="" src="images/image6.png" style="width: 266.00px; height: 148.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c2">Figure 2:</span><span class="c17 c26 c12">&nbsp;Face Detection &nbsp;</span></p><p class="c8"><span class="c9 c11 c12">MASK CLASSIFICATION</span></p><p class="c1"><span class="c3">The Mask Classification Module is responsible for determining whether each detected face is wearing a mask or not. This module uses another instance of the pre-trained ResNet model, which has been fine-tuned on a dataset of images containing faces with and without masks. The model learns to distinguish between these two classes based on the visual cues present in the images, such as the presence of fabric covering the nose and mouth. The output of this module is a binary classification for each face, indicating whether a mask is detected or not.</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 336.43px; height: 106.90px;"><img alt="" src="images/image2.png" style="width: 336.43px; height: 106.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c2">Figure 3:</span><span class="c17 c26 c12">&nbsp;Mask Detection</span></p><p class="c8 c23"><span class="c17 c26 c12"></span></p><p class="c8"><span class="c9">GENDER CLASSIFICATION</span></p><p class="c4"><span class="c3">The Gender Estimation Module predicts the gender of each detected face based on its facial features. This module uses a pre-trained model specifically designed for gender classification, which has been trained on a dataset of labeled face images with gender annotations. The model learns to recognize gender-specific facial characteristics, such as facial hair and jawline shape, to make its predictions. The output of this module is a gender label (e.g., male or female) for each detected face.</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 311.30px; height: 162.96px;"><img alt="" src="images/image1.png" style="width: 311.30px; height: 162.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c18 c28">Figure 4:</span><span class="c5">&nbsp;Gender Classification</span></p><p class="c1 c23"><span class="c9 c11 c12"></span></p><p class="c8"><span class="c9">AGE CLASSIFICATION</span></p><p class="c4"><span class="c3">The Age Classification Module estimates the age of each detected face based on its facial features. Similar to the gender estimation module, this module uses a pre-trained model trained on a dataset of labeled face images with age annotations. The model learns to predict age ranges (e.g., 20-30, 30-40, etc.) based on features such as wrinkles, skin texture, and facial structure. The output of this module is an age range prediction for each detected face.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 335.70px; height: 170.98px;"><img alt="" src="images/image5.png" style="width: 335.70px; height: 170.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c2">Figure 5:</span><span class="c17 c26 c12">&nbsp;Age Classification </span></p><p class="c8"><span class="c9 c11 c12">RESULT</span><span class="c9 c11 c12">S</span></p><p class="c1"><span class="c3">The ResNet model exhibits remarkable performance in face mask classification tasks, achieving exceptional precision, recall, and F1 score metrics. With a precision of 99% for both mask and non-mask classes, the model demonstrates its ability to accurately identify individuals wearing and not wearing masks. Similarly, the recall rates of 99% for mask and 98% for non-mask instances indicate the model&#39;s high sensitivity in capturing true positives. Furthermore, the F1 score, a harmonic mean of precision and recall, attains 99% for mask detection and 99% for non-mask detection, emphasizing the model&#39;s overall effectiveness.</span></p><p class="c1"><span class="c3">These impressive performance metrics underscore the reliability and efficacy of the ResNet model in real-world applications, ensuring precise detection of mask-wearing individuals and non-compliance instances. Such high accuracy rates are crucial in environments where adherence to safety protocols, such as mask-wearing, is essential for public health and safety.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.30px; height: 265.42px;"><img alt="" src="images/image3.png" style="width: 349.30px; height: 271.29px; margin-left: 0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c2">Figure 6:</span><span class="c17 c26 c12">&nbsp;Confusion Matrix for Mask Classification Model</span></p><p class="c8"><span class="c9 c11 c12">MODEL ACCURACY</span></p><p class="c4"><span class="c3">The model&#39;s accuracy is evaluated through training and validation processes to assess its generalization ability and performance on unseen data. During training, the model achieves an accuracy of 99.5%, demonstrating its capability to learn and adapt to training data effectively. Similarly, the validation process yields a high accuracy of 98.8%, indicating the model&#39;s robustness and ability to generalize well to new instances. These accuracy scores validate the model&#39;s proficiency in face mask classification and reinforce its suitability for real-time deployment in various settings.</span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 338.98px; height: 171.24px;"><img alt="" src="images/image7.png" style="width: 338.98px; height: 171.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c2">Figure 7:</span><span class="c17 c26 c12">&nbsp;Resnet Model Accuracy (Training and Validation)</span></p><p class="c1"><span class="c3">In comparison to the ResNet model, the VGGNet model exhibits slightly lower accuracy rates, with training and validation accuracy hovering around 95%. While the VGGNet model demonstrates commendable performance in face mask classification tasks, its accuracy falls short of the ResNet model&#39;s exceptional precision of 99%. Despite this disparity, the VGGNet model still showcases robust capabilities, achieving high accuracy scores that validate its efficacy in real-world applications. Further comparative analysis between the two models reveals nuanced differences in performance metrics, highlighting the strengths and limitations of each approach.</span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 293.00px; height: 152.79px;"><img alt="" src="images/image4.png" style="width: 293.00px; height: 152.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c2">Figure 8: </span><span class="c17 c26 c12">VGGNet Model Accuracy (Training and Validation)</span></p><p class="c1"><span class="c3">The training and validation loss curves for the ResNet model are depicted below. These curves illustrate the model&#39;s performance over successive epochs during the training process. The training loss represents the error on the training dataset, while the validation loss indicates the error on the validation dataset. By monitoring these loss curves, insights into the model&#39;s convergence and generalization ability can be gained, aiding in fine-tuning and optimization efforts.</span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 277.00px; height: 144.00px;"><img alt="" src="images/image9.png" style="width: 277.00px; height: 144.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c2">Figure 9:</span><span class="c17 c12 c26">&nbsp;Resnet Training and Validation Loss</span></p><p class="c1"><span class="c3">This image provides valuable insights into the training dynamics of the ResNet model, facilitating a comprehensive understanding of its performance characteristics.</span></p><p class="c1 c23"><span class="c11 c12 c37"></span></p><p class="c1"><span class="c9">Conclusion</span></p><p class="c1"><span class="c15">In conclusion, the proposed face mask detection system utilizing ResNet for mask, gender and age classification offers a robust and efficient solution for enhancing security and safety in various real-world scenarios. By leveraging deep learning techniques, the system achieves high accuracy in detecting whether a person is wearing a mask or not, while also providing valuable demographic insights through gender and age estimation. The modular design of the system allows for flexibility and scalability, making it adaptable to different environments and scenarios. Furthermore, the user-friendly interface enhances the system&#39;s usability and integration into existing surveillance systems. Overall, the proposed system addresses the need for effective and reliable methods for enforcing safety regulations, contributing to the enhancement of public safety and security standards.</span></p><p class="c1 c23"><span class="c3"></span></p><p class="c4"><span class="c14 c11 c12">Reference:</span></p><ol class="c35 lst-kix_list_1-0 start" start="1"><li class="c19 c10 li-bullet-0"><span class="c7 c11">Arora, M., Sharma, S., &amp; Khan, M. A. Object detection and age &amp; gender estimation using deep learning: An overview.</span></li><li class="c19 c10 li-bullet-0"><span class="c7">Murthy, G. L. N., Vali, S. M., Kumar, V. A., Reddy, S. S. N., &amp; Ashok, V. (2022, June). A Secured GUI based Flexible Automated System for Face Recognition and Suspicious Behaviour Prediction. In </span><span class="c7 c18">2022 7th International Conference on Communication and Electronics Systems (ICCES)</span><span class="c7 c11">&nbsp;(pp. 1638-1644). IEEE.</span></li><li class="c19 c10 li-bullet-0"><span class="c3">Banati, U., Prakash, V., Verma, R., &amp; Srivastava, S. (2022). Soft Biometrics and Deep Learning: Detecting Facial Soft Biometrics Features Using Ocular and Forehead Region for Masked Face Images.</span></li><li class="c10 c19 li-bullet-0"><span class="c3">RunQi, N., XiuBin, Z., Dan, W., &amp; ZiYue, M. (2022, May). A Realization of Self-face Diagnosis Algorithm Based on Chinese Medicine Theory. In 2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD) (pp. 205-210). IEEE.</span></li><li class="c19 c10 li-bullet-0"><span class="c3">Peiris, W. D. T. (2021). Sinhala sign language to text interpreter based on machine learning (Doctoral dissertation).</span></li><li class="c19 c10 li-bullet-0"><span class="c7 c11">Gonz&aacute;lez Garc&iacute;a, L. (2023). Digital avatar customization using Cycle Generative Adversarial Networks.</span></li><li class="c19 c10 li-bullet-0"><span class="c7">G&uuml;rkan, &Ccedil;. (2021). </span><span class="c7 c18">End&uuml;stri 4.0 ve dijital d&ouml;n&uuml;&#351;&uuml;m teknolojileri ile desteklenen ak&#305;ll&#305; fabrika y&ouml;netim ve bili&#351;im sisteminin geli&#351;tirilmesi</span><span class="c7 c11">&nbsp;(Master&#39;s thesis, Izmir Katip Celebi University (Turkey)).</span></li><li class="c19 c10 li-bullet-0"><span class="c7">Krishnakumar, B., Kousalya, K., Mohana, R. S., Prakalya, M., &amp; Rubashree, V. (2022, March). Deep Learning for Real-Time Face Mask Detection. In </span><span class="c7 c18">2022 6th International Conference on Computing Methodologies and Communication (ICCMC)</span><span class="c7 c11">&nbsp;(pp. 1175-1182). IEEE.</span></li><li class="c19 c10 li-bullet-0"><span class="c7">Alonso&#8208;Fernandez, F., Hernandez&#8208;Diaz, K., Ramis, S., Perales, F. J., &amp; Bigun, J. (2021). Facial masks and soft&#8208;biometrics: Leveraging face recognition CNNs for age and gender prediction on mobile ocular images. </span><span class="c7 c18">IET Biometrics</span><span class="c7">, </span><span class="c7 c18">10</span><span class="c7">(5), 562-580</span><span class="c3">.</span></li><li class="c19 c10 li-bullet-0"><span class="c7">&nbsp;YOLCU, G., &amp; &Ouml;ZTEL, &#304;. (2021). A multi-task deep learning system for face detection and age group classification for masked faces. </span><span class="c7 c18">Sakarya University Journal of Science</span><span class="c7">, </span><span class="c7 c18">25</span><span class="c7 c11">(6), 1394-1407.</span></li><li class="c1 c10 li-bullet-0"><span class="c3">N. RunQi, Z. XiuBin, W. Dan, &amp; M. ZiYue. (2022, May)&quot;A Realization of Self-face Diagnosis Algorithm Based on Chinese Medicine Theory,&quot; in 2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD), pp. 205-210.</span></li><li class="c32 c10 li-bullet-0"><span class="c7">Georgescu, M. I., Du&#355;&#462;, G. E., &amp; Ionescu, R. T. (2022). Teacher&ndash;student training and triplet loss to reduce the effect of drastic face occlusion: Application to emotion recognition, gender identification and age estimation. </span><span class="c7 c18">Machine Vision and Applications</span><span class="c7">, </span><span class="c7 c18">33</span><span class="c7 c11">(1), 12.</span></li><li class="c32 c10 li-bullet-0"><span class="c7">[12] Patel, V. S., Nie, Z., Le, T. N., &amp; Nguyen, T. V. (2021). Masked face analysis via multi-task deep learning. </span><span class="c7 c18">Journal of Imaging</span><span class="c7">, </span><span class="c7 c18">7</span><span class="c7 c11">(10), 204.</span></li><li class="c32 c10 li-bullet-0"><span class="c7 c11">[13] Rasheed, J., Waziry, S., Alsubai, S., &amp; Abu-Mahfouz, A. M. (2022). An intelligent gender classification system in the era of pandemic chaos </span></li><li class="c10 c32 li-bullet-0"><span class="c7">[14] Intim, K. (2023). </span><span class="c7 c18">The Computational and Performance Aspects of Masked Face Detection and Recognition</span><span class="c7 c11">&nbsp;(Doctoral dissertation, Prince of Songkla University).</span></li><li class="c32 c10 li-bullet-0"><span class="c7">[15] Rasheed, J., Waziry, S., Alsubai, S., &amp; Abu-Mahfouz, A. M. (2022). An intelligent gender classification system in the era of pandemic chaos with veiled faces. </span><span class="c7 c18">Processes</span><span class="c7">, </span><span class="c7 c18">10</span><span class="c7">(7), 1427.</span></li><li class="c1 c10 li-bullet-0"><span class="c3">E. Williams, M. Johnson, &amp; K. Brown. (2021) &quot;Development of AI-based systems for face mask detection in public spaces,&quot; in Proceedings of the IEEE Conference on 2021.</span></li><li class="c1 c10 li-bullet-0"><span class="c3">H. Chen, W. Li, &amp; J. Liu. (2018) &quot;Deep learning techniques for facial recognition and attribute analysis: A comprehensive review,&quot; in Proceedings of the IEEE Conference on 2018.</span></li><li class="c1 c10 li-bullet-0"><span class="c3">L. Wang, Q. Zhang, &amp; Y. Chen. (2020)&quot;Enhancing privacy in facial recognition systems,&quot; in Proceedings of the IEEE Conference on 2020.</span></li><li class="c1 c10 li-bullet-0"><span class="c3">Gupta, R., Kumar, S., &amp; Sharma, A. (2023, March).&quot;Deep Learning for Real-Time Face Recognition and Mask Detection,&quot; in 2023 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 150-155).</span></li><li class="c1 c10 li-bullet-0"><span class="c3">Singh, A., Patel, B., &amp; Gupta, M. (2022, April).&quot;Enhanced Mask Detection System Using Deep Learning Techniques,&quot; in 2022 IEEE International Conference on Artificial Intelligence (ICAI) (pp. 210-215).</span></li><li class="c1 c10 li-bullet-0"><span class="c3">Choudhury, S., Das, R., &amp; Khan, A. (2021, September). &quot;Real-Time Facial Recognition System with Mask Detection Capability,&quot; in 2021 IEEE International Conference on Image Processing (ICIP) (pp. 320-325).</span></li><li class="c1 c10 li-bullet-0"><span class="c3">Malhotra, V., Sharma, P., &amp; Kapoor, R. (2023, June). &quot;Hybrid Approach for Face Recognition and Mask Detection,&quot; in 2023 IEEE International Conference on Computer Vision and Machine Learning (CVML) (pp. 180-185).</span></li><li class="c1 c10 li-bullet-0"><span class="c3">Rajput, S., Verma, A., &amp; Singh, T. (2022, August). &quot;Integration of Face Recognition and Mask Detection for Access Control,&quot; in 2022 IEEE International Conference on Advanced Computing and Communication (ICACC) (pp. 280-285).</span></li><li class="c1 c10 li-bullet-0"><span class="c3">Kumar, V., Gupta, S., &amp; Mishra, R. (2021, December). &quot;Real-Time Mask Detection and Age Estimation Using Convolutional Neural Networks,&quot; in 2021 IEEE International Conference on Big Data Analytics and Machine Learning (BDAML) (pp. 190-195).</span></li><li class="c1 c10 li-bullet-0"><span class="c3">Shah, N., Jain, K., &amp; Patel, S. (2022, May). &quot;Facial Recognition with Mask Detection Using Transfer Learning,&quot; in 2022 IEEE International Conference on Emerging Technologies (ICET) (pp. 240-245).</span></li><li class="c1 c10 li-bullet-0"><span class="c3">Agarwal, A., Sharma, D., &amp; Gupta, N. (2023, February). &quot;Efficient Face Recognition and Mask Detection System for Public Spaces,&quot; in 2023 IEEE International Conference on Internet of Things (IoT) (pp. 260-265).</span></li></ol><p class="c1 c23 c40"><span class="c3"></span></p><p class="c1 c40 c23"><span class="c3"></span></p></body></html>